{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "from datasets import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "# stats_cal(datasets_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Download cnn_dailymail datasets and view source/target pairs as starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/home/jli/working_dir/datasets_cache/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "Reusing dataset cnn_dailymail (/home/jli/working_dir/datasets_cache/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "Reusing dataset cnn_dailymail (/home/jli/working_dir/datasets_cache/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "Using custom data configuration 1.2.0\n",
      "Reusing dataset xsum (/home/jli/working_dir/datasets_cache/xsum/1.2.0/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n",
      "Using custom data configuration 1.2.0\n",
      "Reusing dataset xsum (/home/jli/working_dir/datasets_cache/xsum/1.2.0/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n",
      "Using custom data configuration 1.2.0\n",
      "Reusing dataset xsum (/home/jli/working_dir/datasets_cache/xsum/1.2.0/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n",
      "Reusing dataset wiki_lingua (/home/jli/working_dir/datasets_cache/wiki_lingua/english/1.1.1/dad5f0710d1bb1a2ed1eb7633802fe147ebd27a3812985f5641bb7aca6ba5c08)\n",
      "Reusing dataset scitldr (/home/jli/working_dir/datasets_cache/scitldr/Abstract/0.0.0/79e0fa75961392034484808cfcc8f37deb15ceda153b798c92d9f621d1042fef)\n",
      "Reusing dataset scitldr (/home/jli/working_dir/datasets_cache/scitldr/Abstract/0.0.0/79e0fa75961392034484808cfcc8f37deb15ceda153b798c92d9f621d1042fef)\n",
      "Reusing dataset scitldr (/home/jli/working_dir/datasets_cache/scitldr/Abstract/0.0.0/79e0fa75961392034484808cfcc8f37deb15ceda153b798c92d9f621d1042fef)\n",
      "Using custom data configuration 3.0.0\n",
      "Reusing dataset billsum (/home/jli/working_dir/datasets_cache/billsum/3.0.0/3.0.0/d1e95173aed3acb71327864be74ead49b578522e4c7206048b2f2e5351b57959)\n",
      "Using custom data configuration 3.0.0\n",
      "Reusing dataset billsum (/home/jli/working_dir/datasets_cache/billsum/3.0.0/3.0.0/d1e95173aed3acb71327864be74ead49b578522e4c7206048b2f2e5351b57959)\n",
      "Using custom data configuration 3.0.0\n",
      "Reusing dataset billsum (/home/jli/working_dir/datasets_cache/billsum/3.0.0/3.0.0/d1e95173aed3acb71327864be74ead49b578522e4c7206048b2f2e5351b57959)\n"
     ]
    }
   ],
   "source": [
    "# load cnn_dailymail\n",
    "datasets_train = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "datasets_test = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "datasets_valid = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation\")\n",
    "# load xsum\n",
    "xsum_train = load_dataset(\"xsum\", \"1.2.0\", split=\"train\")\n",
    "xsum_test = load_dataset(\"xsum\", \"1.2.0\", split=\"test\")\n",
    "xsum_valid = load_dataset(\"xsum\", \"1.2.0\", split=\"validation\")\n",
    "\n",
    "# load wiki_lingua English\n",
    "wiki_lingua_train = load_dataset(\"wiki_lingua\", \"english\", split=\"train\")\n",
    "\n",
    "# load scitldr\n",
    "scitldr_train = load_dataset(\"scitldr\", \"Abstract\", split=\"train\")\n",
    "scitldr_test = load_dataset(\"scitldr\", \"Abstract\", split=\"test\")\n",
    "scitldr_valid = load_dataset(\"scitldr\", \"Abstract\", split=\"validation\")\n",
    "\n",
    "# load billsum\n",
    "billsum_train = load_dataset(\"billsum\", \"3.0.0\", split=\"train\")\n",
    "billsum_test = load_dataset(\"billsum\", \"3.0.0\", split=\"test\")\n",
    "billsum_valid = load_dataset(\"billsum\", \"3.0.0\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wiki_lingua_train[\"article\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp(\"\".join(wiki_lingua_train[\"article\"][0].get(\"summary\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=scitldr_test.to_pandas()\n",
    "len(nlp(ds.source.str.join(\"\")[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the source of cnn_daily mail is 'Article' and the target is 'highlights'. They are both Strings of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Provide a function that gives the input of a dataset and computes: \n",
    "#### 1) Number of samples in train/validation/test. 2) Mean number of tokens (this can be whitespace-splitting for now) for source texts, including standard deviation and median. 3) Mean number of tokens (this can be whitespace-splitting for now) for summary texts, including standard deviation and median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TokenNum(tokens, n):\n",
    "    \"\"\"\n",
    "    Compute number of tokens in each row.\n",
    "    Input: rows of tokens, number of rows.\n",
    "    Output: mean, standard deviation and median of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # lens = np.zeros(n)\n",
    "\n",
    "    # for i in range(n):\n",
    "    #     lens[i] = len(nlp(tokens[i]))\n",
    "\n",
    "    lens = [len(nlp(token)) for token in tokens]\n",
    "    lens = np.array(lens)\n",
    "    mean = np.mean(lens)\n",
    "    median = np.median(lens)\n",
    "    std = np.std(lens)\n",
    "\n",
    "    return mean, median, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_cal(dataset):\n",
    "    num_samples = dataset.num_rows\n",
    "    src_mean, src_median, src_std = get_TokenNum(dataset[\"source\"],num_samples)\n",
    "    tg_mean, tg_median, tg_std = get_TokenNum(dataset[\"target\"],num_samples)\n",
    "\n",
    "    print(\"Number of samples: \",num_samples)\n",
    "    print(\"mean, median and standard deviation of number of tokens of Source: \",src_mean, src_median, src_std)\n",
    "    print(\"mean, median and standard deviation of number of tokens of Source: \",tg_mean, tg_median, tg_std)\n",
    "    # return num_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_datasets(dataset):\n",
    "    dataset = dataset.rename_column(dataset.column_names[0],\"source\")\n",
    "    dataset = dataset.rename_column(dataset.column_names[1],\"target\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets_test[\"source\"][0].split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp(datasets_test[\"source\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00b683f7247be56873f76d0b0d257e8fa6bb4a1e3e68fabe0e16b66c988ff868"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
