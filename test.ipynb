{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple, NamedTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_datasets(dataset):\n",
    "    dataset = dataset.rename_column(dataset.column_names[0], \"source\")\n",
    "    dataset = dataset.rename_column(dataset.column_names[1], \"target\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def spacy_token(samples: List[str]) -> NamedTuple:\n",
    "    \"\"\"\n",
    "    Compute number of tokens in each row.\n",
    "    Input: rows of tokens, number of rows.\n",
    "    Output: mean, standard deviation and median of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    stats = namedtuple(\"stats\", \"mean median std\")\n",
    "\n",
    "    # lens = [len(nlp(token)) for token in tqdm(tokens)]\n",
    "    tokens = list(tqdm(nlp.pipe(samples, n_process=8), total=len(samples)))\n",
    "    lens = [len(token) for token in (iter(tokens))]\n",
    "\n",
    "    lens = np.array(lens)\n",
    "    stats.lens = lens\n",
    "    stats.mean = np.mean(lens)\n",
    "    stats.median = np.median(lens)\n",
    "    stats.std = np.std(lens)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def whitespace_token(samples: List[str]) -> NamedTuple:\n",
    "\n",
    "    stats = namedtuple(\"stats\", \"mean median std lens\")\n",
    "\n",
    "    lens = samples.str.split().str.len()\n",
    "\n",
    "    lens = np.array(lens)\n",
    "    stats.lens = lens\n",
    "    stats.mean = np.mean(lens)\n",
    "    stats.median = np.median(lens)\n",
    "    stats.std = np.std(lens)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def format_tuning(dataset):\n",
    "    try:\n",
    "        if (\n",
    "            dataset.features[\"source\"].feature._type == \"Value\"\n",
    "        ):  # One row of source is a line in article.\n",
    "            dataset = dataset.to_pandas()\n",
    "\n",
    "    except AttributeError:\n",
    "        if len(dataset.features[\"source\"].feature) > 1:\n",
    "            dataset = pd.DataFrame(dataset[\"source\"])\n",
    "            dataset = dataset.rename(\n",
    "                columns={\"document\": \"source\", \"summary\": \"target\"}\n",
    "            )\n",
    "\n",
    "    dataset[\"source\"] = dataset[\"source\"].str.join(\"\")\n",
    "    dataset[\"target\"] = dataset[\"target\"].str.join(\"\")\n",
    "    return dataset\n",
    "\n",
    "def remove_empty(df):\n",
    "    df.replace(to_replace=r'^\\s*$',value=np.nan,regex=True,inplace=True)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def stats_cal(\n",
    "    dataset,\n",
    "    dataset_name: str,\n",
    "    tokenization_method: str = \"whitespace\",\n",
    "    stats_to_compute: List[str] = [\n",
    "        \"SampleNum\",\n",
    "        \"mean\",\n",
    "        \"median\",\n",
    "        \"std\",\n",
    "        \"compression_ratio\",\n",
    "    ],\n",
    ") -> NamedTuple:\n",
    "\n",
    "    stats_attr_src = namedtuple(\"stats_attr\", stats_to_compute)\n",
    "    stats_attr_tg = namedtuple(\"stats_attr\", stats_to_compute)\n",
    "    stats = namedtuple(\"stats\", \"src tg\")\n",
    "    stats.src = stats_attr_src\n",
    "    stats.tg = stats_attr_tg\n",
    "    stats.src.SampleNum = dataset.num_rows\n",
    "    stats.tg.SampleNum = dataset.num_rows\n",
    "\n",
    "    # Use pandas dataframe to process data and remove samples that contain empty strings. \n",
    "\n",
    "    if dataset.features[\"source\"]._type == \"Value\":  # One row of source is one article.\n",
    "            dataset = dataset.to_pandas()\n",
    "            \n",
    "    elif (\n",
    "        dataset.features[\"source\"]._type == \"Sequence\"\n",
    "    ):  # One row of source is a line in article or combined with article, summary and id.\n",
    "        dataset = format_tuning(dataset)\n",
    "    \n",
    "    dataset = remove_empty(dataset)\n",
    "    print(dataset.shape)\n",
    "    \n",
    "    if tokenization_method == \"whitespace\":\n",
    "        stats_src = whitespace_token(dataset[\"source\"])\n",
    "        stats_tg = whitespace_token(dataset[\"target\"])\n",
    "    elif tokenization_method == \"spacy\":\n",
    "        stats_src = spacy_token(dataset[\"source\"])\n",
    "        stats_tg = spacy_token(dataset[\"target\"])\n",
    "\n",
    "    if \"SampleNum\" in stats_to_compute:\n",
    "        print(\n",
    "            f\"[{dataset_name}] Number of samples of article or summary: {stats.src.SampleNum}\"\n",
    "        )\n",
    "    if \"mean\" in stats_to_compute:\n",
    "        stats.src.mean = stats_src.mean\n",
    "        stats.tg.mean = stats_tg.mean\n",
    "        print(\n",
    "            f\"[{dataset_name}] Mean of article & summary: {stats.src.mean:.2f}, {stats.tg.mean:.2f}\"\n",
    "        )\n",
    "    if \"median\" in stats_to_compute:\n",
    "        stats.src.median = stats_src.median\n",
    "        stats.tg.median = stats_tg.median\n",
    "        print(\n",
    "            f\"[{dataset_name}] Median of article & summary: {stats.src.median:.2f}, {stats.tg.median:.2f}\"\n",
    "        )\n",
    "    if \"std\" in stats_to_compute:\n",
    "        stats.src.std = stats_src.std\n",
    "        stats.tg.std = stats_tg.std\n",
    "        print(\n",
    "            f\"[{dataset_name}] Standard Deviation of article & summary: {stats.src.std:.2f}, {stats.tg.std:.2f}\"\n",
    "        )\n",
    "    if \"compression_ratio\" in stats_to_compute:\n",
    "        stats.src.compression_ratio = np.mean(stats_src.lens / stats_tg.lens)\n",
    "        stats.tg.compression_ratio = stats.src.compression_ratio\n",
    "        print(\n",
    "            f\"[{dataset_name}] ratio of article/summary: {stats.src.compression_ratio:.2f}\"\n",
    "        )\n",
    "\n",
    "    # print(len(stats_tg.lens[np.where(stats_tg.lens == 0)]))\n",
    "    return stats\n",
    "\n",
    "\n",
    "def print_stats(\n",
    "    dataset, dataset_name: str, tokenization_method: str = \"whitespace\"\n",
    ") -> None:\n",
    "\n",
    "    print(f\"********{tokenization_method}********\")\n",
    "    stats = stats_cal(dataset, dataset_name, tokenization_method)\n",
    "\n",
    "\n",
    "def load_data(dataset_name: str, version: str, split_: str = \"train\"):\n",
    "    dataset = load_dataset(dataset_name, version, split=split_)\n",
    "    if dataset_name == \"wiki_lingua\":\n",
    "        dataset = dataset.rename_column(\"article\", \"source\")\n",
    "    elif dataset_name == \"scitldr\":\n",
    "        pass\n",
    "    else:\n",
    "        dataset = rename_datasets(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load(\n",
    "    disable=(\"tok2vec\", \"tagger\", \"lemmatizer\", \"ner\")\n",
    ")  # Disabling components for only tokenization use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train = load_data(\"cnn_dailymail\", \"3.0.0\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/home/jli/working_dir/datasets_cache/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    }
   ],
   "source": [
    "cnn_test = load_data(\"cnn_dailymail\", \"3.0.0\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_test = load_data(\"xsum\", \"1.2.0\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********whitespace********\n",
      "(11490, 3)\n",
      "[cnn_dailymail] Number of samples of article or summary: 11490\n",
      "[cnn_dailymail] Mean of article & summary: 683.51, 55.01\n",
      "[cnn_dailymail] Median of article & summary: 613.00, 51.00\n",
      "[cnn_dailymail] Standard Deviation of article & summary: 348.39, 22.52\n",
      "[cnn_dailymail] ratio of article/summary: 13.45\n"
     ]
    }
   ],
   "source": [
    "print_stats(cnn_test, \"cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********whitespace********\n",
      "(11333, 3)\n",
      "[xsum] Number of samples of article or summary: 11334\n",
      "[xsum] Mean of article & summary: 376.18, 21.10\n",
      "[xsum] Median of article & summary: 295.00, 21.00\n",
      "[xsum] Standard Deviation of article & summary: 308.22, 5.32\n",
      "[xsum] ratio of article/summary: 18.98\n"
     ]
    }
   ],
   "source": [
    "print_stats(xsum_test, \"xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********spacy********\n",
      "(11333, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11333/11333 [01:27<00:00, 129.88it/s]\n",
      "100%|██████████| 11333/11333 [00:10<00:00, 1080.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[xsum] Number of samples of article or summary: 11334\n",
      "[xsum] Mean of article & summary: 457.51, 23.95\n",
      "[xsum] Median of article & summary: 359.00, 24.00\n",
      "[xsum] Standard Deviation of article & summary: 377.91, 6.00\n",
      "[xsum] ratio of article/summary: 20.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_stats(xsum_test, \"xsum\", \"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = list(tqdm(nlp.pipe(xsum_train[\"document\"], n_process=8), total=len(xsum_train[\"document\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.isnull().values == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_en = load_dataset(\"GEM/wiki_lingua\", \"en\", split=\"train\")\n",
    "# df = pd.DataFrame(wiki_en)\n",
    "# df.replace(to_replace=r'^\\s*$',value=np.nan,regex=True,inplace=True)\n",
    "# display.display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': Value(dtype='string', id=None),\n",
       " 'target': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00b683f7247be56873f76d0b0d257e8fa6bb4a1e3e68fabe0e16b66c988ff868"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
